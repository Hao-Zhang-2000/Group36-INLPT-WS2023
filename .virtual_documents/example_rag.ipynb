import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model

from data.databases import NumpyDataBase
from data.embedding_models import EmbeddingModelMiniLML6
from training.utils import LLAMA_TEMPLATES, MISTRAL_TEMPLATES, system_message, format_user_message, format_conversation
import json


with open("data/data.json", "r") as f:
    data = json.load(f)
embedding_model = EmbeddingModelMiniLML6()
database = NumpyDataBase(data=data, embedding_model=embedding_model)


base_model = {
    "llama": {
        "path": "meta-llama/Llama-2-7b-chat-hf",
        "save-path": "MediRAG-LLaMA",
        "templates": LLAMA_TEMPLATES
    },
    "mistral": {
        "path": "mistralai/Mistral-7B-Instruct-v0.2",
        "save-path": "MediRAG-Mistral",
        "templates": MISTRAL_TEMPLATES
    },
    "meditron": {
        "path": "epfl-llm/meditron-7b",
        "save-path": "MediRAG-Meditron2",
        "templates": LLAMA_TEMPLATES
    }
}["mistral"]


quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)
model = AutoModelForCausalLM.from_pretrained(
    base_model['path'],
    quantization_config=quant_config,
    #trust_remote_code=True
)
model.config.use_cache = False


model = PeftModel.from_pretrained(model, f"training/{base_model['save-path']}/")
model = prepare_model_for_kbit_training(model)

tokenizer  = AutoTokenizer.from_pretrained(base_model['path'], use_fast=False)
tokenizer.pad_token = tokenizer.eos_token


user_query = "Are there any Artificial Neural Networks that can detect or predict cancer?"
relevant_abstracts = database.retrieve_by_query(user_query)


conversation = [
    {
        "role": "system", 
        "content": system_message
    },
    {
        "role": "user",
        "content": format_user_message(user_query, relevant_abstracts)
    }
]
_, input_ids = format_conversation(conversation, base_model['templates'], tokenizer, training=False)


response = model.generate(input_ids=torch.tensor(input_ids).unsqueeze(0), max_new_tokens=512)


print(tokenizer.decode(response.squeeze().tolist()))






